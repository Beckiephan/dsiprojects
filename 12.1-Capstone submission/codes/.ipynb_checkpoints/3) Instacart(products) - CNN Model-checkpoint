{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Capstone Project - Instacart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Instacart being a grocery delivery and pick-up service, offers its services through the website and mobile app. Being an e-commerce business especially now, during the Covid period, it is important to maintain user experience and ease in finding products on their online store. As a data analyst with Instacart, it has come to our attention that there are quite a lot of missing information of which department and aisles a product belongs to. As products are categorized into departments, users could search for products by departments. With missing departments, those products would not appear if users were to search by department. To solve this problem, we will build a model that would automatically take the product name and map them to their most likely department which would ensure all items are mapped to their most closely related department. This would not only ease users search for items and prevent any omission of products if users search by departments, which may result in higher sales as well, it would give a more complete and organized inventory and help make reporting more efficient, complete and accurate, for example, reporting the department-wise sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "### Contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:05.913691Z",
     "start_time": "2021-01-27T08:01:05.899616Z"
    }
   },
   "outputs": [],
   "source": [
    "#import all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords as stops\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.metrics import r2_score, confusion_matrix, plot_roc_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from MulticoreTSNE import MulticoreTSNE as tsne\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import utils\n",
    "from keras import layers\n",
    "from plot_keras_history import plot_history\n",
    "from tensorflow.python.keras.callbacks import History\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.002123Z",
     "start_time": "2021-01-27T08:01:05.915646Z"
    }
   },
   "outputs": [],
   "source": [
    "## Loading the clean products csv\n",
    "products_df = pd.read_csv('../datasets/products_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.008675Z",
     "start_time": "2021-01-27T08:01:06.004561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49688, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we will be working with 67 out of the 134 aisles due to computational limitations. We will take the more well represented aisles, with at least 305 products in each aisle. In this case, we did not use SMOTEENN as we will just be created artificial samples of each minority classes by over-sampling and we may be taking away some information by under-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.015888Z",
     "start_time": "2021-01-27T08:01:06.010620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df['department_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.025043Z",
     "start_time": "2021-01-27T08:01:06.017539Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49688, 9)\n",
      "(1258, 9)\n"
     ]
    }
   ],
   "source": [
    "print(products_df.shape)\n",
    "print(products_df[products_df['department_name']=='missing'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation of the model will be done on a set of data that have their relative department name, we will do a train_test_split on this set to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.044904Z",
     "start_time": "2021-01-27T08:01:06.026530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48430, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>aisle_name</th>\n",
       "      <th>department_id</th>\n",
       "      <th>department_name</th>\n",
       "      <th>length_product_name</th>\n",
       "      <th>product_name_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, product_id, product_name, aisle_id, aisle_name, department_id, department_name, length_product_name, product_name_words]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data consist of all those departments that are not missing\n",
    "Train = products_df[(products_df['department_name'] != 'missing')]\n",
    "print(Train.shape)\n",
    "Train[Train['department_name'] == 'missing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen production model will be tested on with unseen data (missing departmentes) which will be the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.059964Z",
     "start_time": "2021-01-27T08:01:06.046584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1258, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>aisle_name</th>\n",
       "      <th>department_id</th>\n",
       "      <th>department_name</th>\n",
       "      <th>length_product_name</th>\n",
       "      <th>product_name_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>Ultra Antibacterial Dish Liquid</td>\n",
       "      <td>100</td>\n",
       "      <td>missing</td>\n",
       "      <td>21</td>\n",
       "      <td>missing</td>\n",
       "      <td>4</td>\n",
       "      <td>ultra antibacterial dish liquid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>Organic Honeycrisp Apples</td>\n",
       "      <td>100</td>\n",
       "      <td>missing</td>\n",
       "      <td>21</td>\n",
       "      <td>missing</td>\n",
       "      <td>3</td>\n",
       "      <td>organic honeycrisp apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  product_id                     product_name  aisle_id  \\\n",
       "37          37          38  Ultra Antibacterial Dish Liquid       100   \n",
       "71          71          72        Organic Honeycrisp Apples       100   \n",
       "\n",
       "   aisle_name  department_id department_name  length_product_name  \\\n",
       "37    missing             21         missing                    4   \n",
       "71    missing             21         missing                    3   \n",
       "\n",
       "                 product_name_words  \n",
       "37  ultra antibacterial dish liquid  \n",
       "71         organic honeycrisp apple  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing data consist of all those aisles and departments that are missing\n",
    "Test = products_df[(products_df['department_name'] == 'missing')]\n",
    "print(Test.shape)\n",
    "Test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.065617Z",
     "start_time": "2021-01-27T08:01:06.063327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign X and y value\n",
    "X = Train['product_name']\n",
    "y = Train['department_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T03:01:09.449906Z",
     "start_time": "2021-01-19T03:01:09.447855Z"
    }
   },
   "source": [
    "There are 21 departments, and we are interested to classify all products to their departments as accurately as possible. This would be a multi-class classification and all classes in this case would be of equal importance. There is no need to do StandardScaler preprocessing as the vectors will have the same scale. We will do some preprocessing on the data like train_test_split and preparing the vocabulary using different methods like CountVectorizer, TfidfVectorizer, Word2Vec and Doc2Vec. We will be using macro averaging(M) and not the micro averaging(µ) to measure the overall performance as macro averaging treats all classes equally. Since we are interested in all the classes, the metrics for evaluating our models in which we will be interested in are:\n",
    "\n",
    "1) **Fscore(m)**: mean of precision(m) and recall(m)\n",
    "* Precision(m): the average proportion of positive predictions that was actually correct per class\n",
    "    > sum(TP/(TP+FP))/number of classes \n",
    "* Recall(m): proportion of actual positives that was predicted correctly per class\n",
    "    > sum(TP/(TP+FN))/number of classes\n",
    "\n",
    "2) **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.099274Z",
     "start_time": "2021-01-27T08:01:06.068024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing train and validation set\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(X,\n",
    "                                              y,\n",
    "                                              test_size=0.25,\n",
    "                                              random_state=42,\n",
    "                                              stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.106950Z",
     "start_time": "2021-01-27T08:01:06.101831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36322,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24248              Jalapeno Chomperz Crunchy Seaweed Chips\n",
       "48754                      Pesto Alla Genovese Pasta Sauce\n",
       "9317                        California Peach Lowfat Yogurt\n",
       "16009                   Mocha with Almond Milk Iced Coffee\n",
       "6592     Geranium Scented Multi-Surface Everyday Spray ...\n",
       "Name: product_name, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "Xtrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.113608Z",
     "start_time": "2021-01-27T08:01:06.108903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12108,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19437                               Chunky Beef Soup\n",
       "28823                                    Onion Rings\n",
       "17736    Gluten Free Red Velvet & Tuxedo Cupcake Duo\n",
       "34609                               Miso Ramen Broth\n",
       "16210                     3D White Pulsar Toothbrush\n",
       "Name: product_name, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Xval.shape)\n",
    "Xval[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.120177Z",
     "start_time": "2021-01-27T08:01:06.115290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36322,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24248    19\n",
       "48754     9\n",
       "9317     16\n",
       "16009     7\n",
       "6592     17\n",
       "Name: department_id, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ytrain.shape)\n",
    "ytrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.126720Z",
     "start_time": "2021-01-27T08:01:06.121836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12108,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19437    15\n",
       "28823     1\n",
       "17736     3\n",
       "34609    15\n",
       "16210    11\n",
       "Name: department_id, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(yval.shape)\n",
    "yval[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.132969Z",
     "start_time": "2021-01-27T08:01:06.128548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4922"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.138924Z",
     "start_time": "2021-01-27T08:01:06.134757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.144847Z",
     "start_time": "2021-01-27T08:01:06.140420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yval.value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:06.150254Z",
     "start_time": "2021-01-27T08:01:06.146292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yval.value_counts().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any models, the products will be classified correctly at about 14%. This is the baseline accuracy, which means if all product names were to be classified into our majority class, it would be 14% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:54:47.346180Z",
     "start_time": "2021-01-27T07:54:47.327815Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11    0.135510\n",
       "19    0.129343\n",
       "13    0.110897\n",
       "7     0.090138\n",
       "1     0.082732\n",
       "Name: department_id, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.value_counts(normalize=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Model - CountVectorizer, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LogisticRegression model will be used as our benchmark model as it is a simple yet effective model. We will use the pipeline and gridsearch to find the best parameters for LogisticRegression. For the benchmark, we will use CountVectorizer to vectorize our vocabulary since it is also the most simple counting of number of each word. From our EDA, the products in each classes are distinctive enough between each other, therefore do not require over and unders sampling. \n",
    "\n",
    "[Solver](https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:57:08.608729Z",
     "start_time": "2021-01-27T03:57:08.549540Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create pipe\n",
    "pipe_lr_cvec = Pipeline([('cvec', CountVectorizer()),\n",
    "                         ('lr', LogisticRegression(random_state=42, max_iter=1000))])\n",
    "\n",
    "# Define dictionary of hyperparameters.\n",
    "pipe_lr_cvec_params = {\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'cvec__max_features': [40_000, 80_000] #reduce the amount of features as there are alot of features at 160,000\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV object.\n",
    "pipe_lr_cvec_gs = GridSearchCV(pipe_lr_cvec,\n",
    "                               pipe_lr_cvec_params,\n",
    "                               cv=5,\n",
    "                               verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T04:03:25.705763Z",
     "start_time": "2021-01-27T03:57:12.648114Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV object to the data.\n",
    "pipe_lr_cvec_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f'Finished and best params: {pipe_lr_cvec_gs.best_params_} and best score: {pipe_lr_cvec_gs.best_score_}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:55:57.884746Z",
     "start_time": "2021-01-27T07:55:56.622364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate default CountVectorizer\n",
    "lr_cvec = CountVectorizer(max_features=80_000, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the CountVectorizer to train\n",
    "lr_Xtrain_cvec = pd.DataFrame(lr_cvec.fit_transform(\n",
    "    Xtrain).toarray(), columns=lr_cvec.get_feature_names())\n",
    "\n",
    "# Transform the fitted CountVectorizer to val\n",
    "lr_Xval_cvec = pd.DataFrame(lr_cvec.transform(\n",
    "    Xval).toarray(), columns=lr_cvec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T05:25:52.619169Z",
     "start_time": "2021-01-27T04:04:49.239521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate LogisticRegression model\n",
    "lr_cvec_ = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# fit train cvec data to LogisticRegression model\n",
    "benchmark_model = lr_cvec_.fit(lr_Xtrain_cvec, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark model does better much better than our baseline but it is overfitted, however the validation score is still quite high at 0.84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:44:22.994225Z",
     "start_time": "2021-01-27T07:44:22.603964Z"
    }
   },
   "outputs": [],
   "source": [
    "# saved the model to disk\n",
    "#pickle.dump(benchmark_model, open('../models/benchmark_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:56:01.420906Z",
     "start_time": "2021-01-27T07:56:01.403077Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the model from disk in case crash\n",
    "#benchmark_model = pickle.load(open('../models/benchmark_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:56:47.560440Z",
     "start_time": "2021-01-27T07:56:03.575330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968448873960685\n",
      "0.8427485959696068\n"
     ]
    }
   ],
   "source": [
    "# check the scoring for the train set\n",
    "benckmark_train_score = benchmark_model.score(lr_Xtrain_cvec, ytrain)\n",
    "print(benckmark_train_score)\n",
    "\n",
    "# check the scoring for the validation set\n",
    "print(benchmark_model.score(lr_Xval_cvec, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:57:00.630273Z",
     "start_time": "2021-01-27T07:56:49.445842Z"
    }
   },
   "outputs": [],
   "source": [
    "# predictions of the validation data\n",
    "benchmark_ypred = benchmark_model.predict(lr_Xval_cvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 socre for the benchmark model, Logistic Regression model fitted with CountVectorized features, is 0.75. This will be the main metrics for measuring and comparing the models as this is an imbalance multiclass text classification. Another metric to compare and see the performance of the models is to look at the confusion matrix itself. We can see that most of the features have been predicted accurately to the correct class. However there are still some misclassification. And department_id 10 which is others have none predicted correctly. This may be because it is the smallest department at only 500 products in them, therefore, in comparison it is hard to train for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:02:43.416996Z",
     "start_time": "2021-01-27T08:02:43.404812Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating list of scores\n",
    "model_name = []\n",
    "model_precision = []\n",
    "model_recall = []\n",
    "model_F1scores = []\n",
    "model_train_accuracy = []\n",
    "model_test_accuracy = []\n",
    "\n",
    "# Create function for the scoring on unseen data\n",
    "def scoring(yact, ypred, model, train_score):\n",
    "    precision = precision_score(yact, ypred, average='macro')\n",
    "    recall = recall_score(yact, ypred, average='macro')\n",
    "    fscore = f1_score(yact, ypred, average='macro')\n",
    "    accuracy = accuracy_score(yact, ypred)\n",
    "\n",
    "    print('Macro Precision: {:.2f}'.format(precision))\n",
    "    print('Macro Recall: {:.2f}'.format(recall))\n",
    "    print('Macro F1-score: {:.2f}\\n'.format(fscore))\n",
    "    print('Accuracy: {:.2f}'.format(accuracy))\n",
    "    print(classification_report(yact, ypred, digits=3))\n",
    "    \n",
    "    # Append to the lists\n",
    "    model_name.append(model)\n",
    "    model_precision.append(precision)\n",
    "    model_recall.append(recall)\n",
    "    model_F1scores.append(fscore)\n",
    "    model_train_accuracy.append(train_score)\n",
    "    model_test_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call scoring function\n",
    "scoring(yval, benchmark_ypred, 'benchmark LR-CVEC model', benckmark_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T05:28:51.230064Z",
     "start_time": "2021-01-27T05:28:51.165502Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(sklearn.metrics.confusion_matrix(yval, benchmark_ypred))\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:52:10.875187Z",
     "start_time": "2021-01-27T03:52:10.816392Z"
    }
   },
   "outputs": [],
   "source": [
    "products_df[products_df['department_id']==10].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T06:56:16.074567Z",
     "start_time": "2021-01-25T06:56:15.156291Z"
    }
   },
   "outputs": [],
   "source": [
    "benchmark_model.predict_proba(lr_Xval_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T06:56:56.171781Z",
     "start_time": "2021-01-25T06:56:56.165784Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labels = LabelEncoder()\n",
    "yval_labels = labels.fit_transform(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T06:57:19.289532Z",
     "start_time": "2021-01-25T06:57:13.136295Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(benchmark_model.predict_proba(lr_Xval_cvec)*100, columns=labels.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression Model - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer although similar to CountVectorizer taking into account the number of times each word appears in a document, but uses the term frequency and puts weights on each word for a document. Lets see how well it performs in comparison to the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:47:24.293336Z",
     "start_time": "2021-01-27T07:47:24.262258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create pipe\n",
    "pipe_lr_tfidf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('lr', LogisticRegression(random_state=42, max_iter=1000))])\n",
    "\n",
    "# Define dictionary of hyperparameters.\n",
    "pipe_lr_tfidf_params = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__max_features': [40_000, 80_000] #reduce the amount of features as there are alot of features at 160,000\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV object.\n",
    "pipe_lr_tfidf_gs = GridSearchCV(pipe_lr_tfidf,\n",
    "                                pipe_lr_tfidf_params,\n",
    "                                cv=5,\n",
    "                                verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T05:35:58.920618Z",
     "start_time": "2021-01-27T05:29:29.942729Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV object to the data.\n",
    "pipe_lr_tfidf_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f'Finished and best params: {pipe_lr_tfidf_gs.best_params_} and best score: {pipe_lr_tfidf_gs.best_score_}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:55:02.775285Z",
     "start_time": "2021-01-27T07:55:01.672079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate default TfidfVectorizer\n",
    "lr_tfidf = TfidfVectorizer(max_features=40_000, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the CountVectorizer to train\n",
    "lr_Xtrain_tfidf = pd.DataFrame(lr_tfidf.fit_transform(\n",
    "    Xtrain).toarray(), columns=lr_tfidf.get_feature_names())\n",
    "\n",
    "# Transform the fitted CountVectorizer to val\n",
    "lr_Xval_tfidf = pd.DataFrame(lr_tfidf.transform(\n",
    "    Xval).toarray(), columns=lr_tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T06:29:14.604865Z",
     "start_time": "2021-01-27T05:36:19.470213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate LogisticRegression model\n",
    "lr_tfidf_ = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# fit train cvec data to LogisticRegression model\n",
    "lr_tfidf_model = lr_tfidf_.fit(lr_Xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:44:30.222856Z",
     "start_time": "2021-01-27T07:44:30.183236Z"
    }
   },
   "outputs": [],
   "source": [
    "# saved the model to disk\n",
    "#pickle.dump(lr_tfidf_model, open('../models/lr_tfidf_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:55:06.791250Z",
     "start_time": "2021-01-27T07:55:06.781826Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the model from disk in case crash\n",
    "#lr_tfidf_model = pickle.load(open('../models/lr_tfidf_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model also performs better than the baseline and is less overfitted than the benchmark model which is the logistic Regression fitted with CountVectorized features, and this model performs better on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:55:27.073527Z",
     "start_time": "2021-01-27T07:55:10.485306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.892324211221849\n",
      "0.8289560621076973\n"
     ]
    }
   ],
   "source": [
    "# check the scoring for the train set\n",
    "lr_tfidf_train_score = lr_tfidf_model.score(lr_Xtrain_tfidf, ytrain)\n",
    "print(lr_tfidf_train_score)\n",
    "\n",
    "# check the scoring for the validation set\n",
    "print(lr_tfidf_model.score(lr_Xval_tfidf, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:55:27.944186Z",
     "start_time": "2021-01-27T07:55:27.083637Z"
    }
   },
   "outputs": [],
   "source": [
    "# predictions of the validation data\n",
    "lr_ypred_tfidf = lr_tfidf_model.predict(lr_Xval_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 socre for the lr_tfidf_model, Logistic Regression model fitted with TfidfVectorized features, is 0.74 which is similar to the benchmark model. This will be the main metrics for measuring and comparing the models as this is an imbalance multiclass text classification. Another metric to compare and see the performance of the models is to look at the confusion matrix itself. We can see from the confusion matrix that most of the features have been predicted accurately to the correct class. However there are still some misclassification. And department_id 10 which is others have none predicted correctly. This may be because it is the smallest department at only 500 products in them, therefore, in comparison it is hard to train for this, this is a similar case seen for the benchmark model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:57:07.399728Z",
     "start_time": "2021-01-27T07:57:07.356037Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.79\n",
      "Macro Recall: 0.71\n",
      "Macro F1-score: 0.74\n",
      "\n",
      "Accuracy: 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.777     0.780     0.778      1002\n",
      "           2      0.778     0.153     0.256       137\n",
      "           3      0.875     0.723     0.792       379\n",
      "           4      0.719     0.724     0.722       421\n",
      "           5      0.937     0.848     0.890       263\n",
      "           6      0.684     0.425     0.524       285\n",
      "           7      0.898     0.927     0.912      1091\n",
      "           8      0.986     0.885     0.933       243\n",
      "           9      0.810     0.753     0.780       465\n",
      "          10      0.000     0.000     0.000         9\n",
      "          11      0.828     0.936     0.879      1641\n",
      "          12      0.773     0.692     0.730       227\n",
      "          13      0.755     0.866     0.807      1343\n",
      "          14      0.925     0.792     0.853       279\n",
      "          15      0.790     0.797     0.794       523\n",
      "          16      0.881     0.916     0.898       862\n",
      "          17      0.931     0.875     0.902       771\n",
      "          18      0.932     0.711     0.807       270\n",
      "          19      0.816     0.901     0.856      1566\n",
      "          20      0.772     0.523     0.623       331\n",
      "\n",
      "    accuracy                          0.829     12108\n",
      "   macro avg      0.793     0.711     0.737     12108\n",
      "weighted avg      0.829     0.829     0.823     12108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Call scoring function\n",
    "scoring(yval, lr_ypred_tfidf, 'LR-TFIDF model', lr_tfidf_train_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T06:33:38.873886Z",
     "start_time": "2021-01-27T06:33:38.764829Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(sklearn.metrics.confusion_matrix(yval, lr_ypred_tfidf))\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression Model - Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some weaknesses of CountVectorizer and TfidfVectorizer is firstly, they lose all information about word order. Secondly, the model does not attempt to learn any underlying meaning, and as a consequence, the distance between vectors doesn’t always reflect the difference in meaning. In this case, as each product name is a document, using Doc2Vec to vectorize our words may see some improvement as it represent documents as a vector which would give some meaning to each document(each product name) to each target(department id).\n",
    "[hyperparameter-size](https://stackoverflow.com/questions/34948650/how-should-i-interpret-size-parameter-in-doc2vec-function-of-gensim)\n",
    "[hyperparameter-dm](https://stackoverflow.com/questions/56323377/which-method-dm-or-dbow-works-well-for-document-similarity-using-doc2vec)\n",
    "[dm&size](https://stackoverflow.com/questions/54521323/i-get-more-vectors-than-my-documents-size-gensim-doc2vec)\n",
    "In this case, which makes sense since it is just product names, it does not help the model to do better but in fact seems to make it worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:05:11.917656Z",
     "start_time": "2021-01-27T08:05:11.912804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create cores for Doc2Vec using multiproccessors\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:05:12.670866Z",
     "start_time": "2021-01-27T08:05:12.660834Z"
    }
   },
   "outputs": [],
   "source": [
    "class Doc2VecModel(BaseEstimator):\n",
    "\n",
    "    def __init__(self, dm=0, workers=cores, vector_size=100, min_count=1):\n",
    "        self.d2v_model = None\n",
    "        self.dm = 0\n",
    "        self.workers = workers\n",
    "        self.vector_size = vector_size\n",
    "        self.min_count = min_count\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        # Initialize model\n",
    "        self.d2v_model = Doc2Vec(dm=self.dm, vector_size=self.vector_size, min_count=self.min_count, workers = self.workers, epochs=100, alpha=0.025, min_alpha=0.001)\n",
    "        # Tag docs\n",
    "        tagged_documents = []\n",
    "        for index, row in raw_documents.iteritems():\n",
    "            tag = '{}_{}'.format(\"type\", index)\n",
    "            tokens = row.split()\n",
    "            tagged_documents.append(TaggedDocument(words=tokens, tags=[tag]))\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.d2v_model.build_vocab(tagged_documents)\n",
    "        # Train model\n",
    "        self.d2v_model.train(tagged_documents, total_examples=len(tagged_documents), epochs=self.d2v_model.epochs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        X = []\n",
    "        for index, row in raw_documents.iteritems():\n",
    "            X.append(self.d2v_model.infer_vector(row.split()))\n",
    "        X = pd.DataFrame(X, index=raw_documents.index)\n",
    "        return X\n",
    "\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        self.fit(raw_documents)\n",
    "        return self.transform(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:50:17.586814Z",
     "start_time": "2021-01-27T07:50:11.869429Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] doc2vec__vector_size=100 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-9ce4d029f8f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Fit the GridSearchCV object to the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlr_d2v_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Finished and best params: {lr_d2v_gs.best_params_} and best score: {lr_d2v_gs.best_score_}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    279\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             ):\n\u001b[0;32m--> 229\u001b[0;31m                 X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    230\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-2a3619643a7f>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-2a3619643a7f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Build vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         self.trainables.prepare_weights(\n\u001b[0m\u001b[1;32m    938\u001b[0m             self.hs, self.negative, self.wv, self.docvecs, update=update)\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[0;34m(self, hs, negative, wv, docvecs, update)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;31m# set initial input/projection and hidden weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m(self, hs, negative, wv, docvecs, vocabulary)\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2VecTrainables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_doc_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_doc_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mreset_doc_weights\u001b[0;34m(self, docvecs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             seed = \"%d %s\" % (\n\u001b[1;32m   1214\u001b[0m                 self.seed, Doc2VecKeyedVectors._index_to_doctag(i, docvecs.offset2doctag, docvecs.max_rawint))\n\u001b[0;32m-> 1215\u001b[0;31m             \u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_doctag_trainables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mseeded_vector\u001b[0;34m(self, seed_string, vector_size)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0;34m\"\"\"Get a random vector (but deterministic by seed_string).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;31m# Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m         \u001b[0monce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashfxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xffffffff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_d2v_params = {'doc2vec__vector_size': [100,200]\n",
    "}\n",
    "\n",
    "lr_d2v_pipe = Pipeline([('doc2vec', Doc2VecModel(workers=cores, dm=0)), ('lr', LogisticRegression(random_state=42, max_iter=1000))])\n",
    "\n",
    "lr_d2v_gs = GridSearchCV(lr_d2v_pipe, \n",
    "                        param_grid=lr_d2v_params,\n",
    "                        verbose=2,\n",
    "                        cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the data.\n",
    "lr_d2v_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f'Finished and best params: {lr_d2v_gs.best_params_} and best score: {lr_d2v_gs.best_score_}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:57:22.600562Z",
     "start_time": "2021-01-27T07:57:18.565722Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tagged = [\n",
    "    TaggedDocument(words=word_tokenize(_d.lower()),\n",
    "                   tags=[str(i)])\n",
    "    for i, _d in enumerate(Xtrain)\n",
    "]\n",
    "\n",
    "val_tagged = [\n",
    "    TaggedDocument(words=word_tokenize(_d.lower()),\n",
    "                   tags=[str(i)])\n",
    "    for i, _d in enumerate(Xval)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:57:22.605774Z",
     "start_time": "2021-01-27T07:57:22.602454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['jalapeno', 'chomperz', 'crunchy', 'seaweed', 'chips'], tags=['0']),\n",
       " TaggedDocument(words=['pesto', 'alla', 'genovese', 'pasta', 'sauce'], tags=['1']),\n",
       " TaggedDocument(words=['california', 'peach', 'lowfat', 'yogurt'], tags=['2']),\n",
       " TaggedDocument(words=['mocha', 'with', 'almond', 'milk', 'iced', 'coffee'], tags=['3']),\n",
       " TaggedDocument(words=['geranium', 'scented', 'multi-surface', 'everyday', 'spray', 'cleaner'], tags=['4'])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:57:29.514188Z",
     "start_time": "2021-01-27T07:57:22.607476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36322/36322 [00:00<00:00, 3434996.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Building Doc2Vec model\n",
    "lr_model_d2v = Doc2Vec(vector_size=200, dm=0, workers=cores)\n",
    "lr_model_d2v.build_vocab([x for x in tqdm(train_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:59:16.078726Z",
     "start_time": "2021-01-27T07:57:29.516530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "#Train Doc2Vec model\n",
    "%time\n",
    "for epoch in range(30):\n",
    "    lr_model_d2v.train(train_tagged, total_examples=lr_model_d2v.corpus_count, epochs=lr_model_d2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:59:16.180818Z",
     "start_time": "2021-01-27T07:59:16.080527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15918581, -0.09649886, -0.13357653, ..., -0.10301879,\n",
       "         0.04833793,  0.47126016],\n",
       "       [-0.0895917 , -0.29506344, -0.07562263, ..., -0.1057526 ,\n",
       "         0.05153437,  0.17427756],\n",
       "       [-0.16252813, -0.00469088,  0.24340214, ...,  0.07630552,\n",
       "         0.21427163,  0.06844027],\n",
       "       ...,\n",
       "       [-0.28914493, -0.15181713,  0.04238746, ..., -0.04216402,\n",
       "         0.09650681, -0.00107136],\n",
       "       [-0.03379864, -0.06143373, -0.08159748, ...,  0.16069898,\n",
       "        -0.18587448, -0.03059678],\n",
       "       [ 0.00691702, -0.17209338,  0.07834499, ...,  0.07638747,\n",
       "         0.01856922,  0.02841053]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract training vectors\n",
    "lr_Xtrain_d2v = np.array([lr_model_d2v.docvecs[str(i)] for i in range(len(train_tagged))])\n",
    "lr_Xtrain_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:59:38.343268Z",
     "start_time": "2021-01-27T07:59:35.170828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.4347479e-02, -1.1478721e-01,  7.5345375e-02, ...,\n",
       "         3.9863557e-02,  4.0686995e-02,  1.0779574e-02],\n",
       "       [-5.2520506e-02, -3.2997712e-02, -5.1174998e-02, ...,\n",
       "         1.8333290e-02, -5.5070232e-05,  3.0561090e-02],\n",
       "       [ 3.4570280e-02, -1.0321204e-01, -3.9730418e-02, ...,\n",
       "         8.3344489e-02, -9.6193757e-03, -5.1851790e-02],\n",
       "       ...,\n",
       "       [-3.4690484e-02, -1.3086987e-02, -8.4680282e-02, ...,\n",
       "         1.7783675e-02, -4.2983633e-02,  6.1793689e-02],\n",
       "       [-1.8378721e-01, -4.9074680e-02, -1.6788326e-02, ...,\n",
       "        -4.7061816e-03,  8.9874705e-03,  4.2181242e-02],\n",
       "       [ 2.0371001e-02,  5.3682741e-02,  4.2887643e-02, ...,\n",
       "        -4.5779806e-02,  6.8014435e-02, -4.1479491e-02]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for test, but for val, we would want to generate vectors for prediction\n",
    "lr_Xval_d2v = np.array([lr_model_d2v.infer_vector(val_tagged[i][0]) for i in range(len(val_tagged))])\n",
    "lr_Xval_d2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model may have performed better than the baseline but in comparison to the Countvectorized and TfidfVectorized Logistic Regression model, it is doing far worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:05:49.549171Z",
     "start_time": "2021-01-27T07:05:40.833956Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train the Model\n",
    "lr_d2v = LogisticRegression(max_iter=10_000)\n",
    "lr_d2v_model = lr_d2v.fit(lr_Xtrain_d2v, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:05:49.554224Z",
     "start_time": "2021-01-27T07:05:49.551199Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "#pickle.dump(lr_d2v_model, open('../models/lr_d2v_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:59:45.745826Z",
     "start_time": "2021-01-27T07:59:45.741096Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the model from disk in case crash\n",
    "#lr_d2v_model = pickle.load(open('../models/lr_d2v_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:59:49.864783Z",
     "start_time": "2021-01-27T07:59:49.794505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05737569517097076\n",
      "Testing accuracy 0.030888668648827222\n",
      "Testing F1 score: 0.012654807834186854\n"
     ]
    }
   ],
   "source": [
    "lr_d2v_train_score = lr_d2v_model.score(lr_Xtrain_d2v, ytrain)\n",
    "lr_ypred_d2v = lr_d2v_model.predict(lr_Xval_d2v)\n",
    "print(lr_d2v_train_score)\n",
    "print('Testing accuracy %s' % accuracy_score(yval, lr_ypred_d2v))\n",
    "print('Testing F1 score: {}'.format(f1_score(yval, lr_ypred_d2v, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 socre for the lr_d2v_model, Logistic Regression model fitted with Doc2Vec features, is 0.28 which much lower than the models before. This will be the main metrics for measuring and comparing the models as this is an imbalance multiclass text classification. Another metric to compare and see the performance of the models is to look at the confusion matrix itself. We can see from the confusion matrix that as compared to the previous models, there are a lot less correctly predicted products to their respective classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:59:56.599356Z",
     "start_time": "2021-01-27T07:59:56.558882Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.05\n",
      "Macro Recall: 0.05\n",
      "Macro F1-score: 0.01\n",
      "\n",
      "Accuracy: 0.03\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.000     0.000     0.000      1002\n",
      "           2      0.000     0.000     0.000       137\n",
      "           3      0.000     0.000     0.000       379\n",
      "           4      0.031     0.204     0.054       421\n",
      "           5      0.027     0.715     0.052       263\n",
      "           6      0.000     0.000     0.000       285\n",
      "           7      0.118     0.027     0.045      1091\n",
      "           8      0.000     0.000     0.000       243\n",
      "           9      0.033     0.004     0.008       465\n",
      "          10      0.000     0.000     0.000         9\n",
      "          11      0.077     0.002     0.004      1641\n",
      "          12      0.000     0.000     0.000       227\n",
      "          13      0.364     0.006     0.012      1343\n",
      "          14      0.000     0.000     0.000       279\n",
      "          15      0.000     0.000     0.000       523\n",
      "          16      0.000     0.000     0.000       862\n",
      "          17      0.026     0.057     0.035       771\n",
      "          18      0.000     0.000     0.000       270\n",
      "          19      0.250     0.001     0.001      1566\n",
      "          20      0.052     0.036     0.043       331\n",
      "\n",
      "    accuracy                          0.031     12108\n",
      "   macro avg      0.049     0.053     0.013     12108\n",
      "weighted avg      0.100     0.031     0.013     12108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/becksphan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/becksphan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Call scoring function\n",
    "scoring(yval, lr_ypred_d2v, 'LR-D2V model', lr_d2v_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:05:53.004426Z",
     "start_time": "2021-01-27T07:05:52.975584Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_d2v_cm = pd.DataFrame(sklearn.metrics.confusion_matrix(yval, lr_d2v_ypred))\n",
    "lr_d2v_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we shall see how it the scores differ with different models but all others remain the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNaiveBayes Model - CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes:\n",
    "P(A/B) =P(A) x P(B/A)P(B)\n",
    "\n",
    "Here we are calculating posterior probability of the class A when predictor B is given to us ie. P(A/B). P(A) is the prior probability of the class. P(B/A) is the likelihood of predictor B given class A probability. P(B) is the prior probability of the predictor B. Calculating these probabilities will help us calculate probabilities of the words in the text\n",
    "\n",
    "Multinomial Naive Bayes simply assumes multinomial distribution for all the pairs i.e. for word counts in documents\n",
    "It is generally used where there are discrete features(for example – word counts in a text classification problem). It generally works with the integer counts which are generated as frequency for each word. All features follow multinomial distribution\n",
    "\n",
    "[Multinomial](https://stats.stackexchange.com/questions/33185/difference-between-naive-bayes-multinomial-naive-bayes)\n",
    "[NaiveBayes & Multinomial](https://analyticsindiamag.com/naive-bayes-why-is-it-favoured-for-text-related-tasks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:06:18.852567Z",
     "start_time": "2021-01-27T07:06:18.847544Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create pipe\n",
    "pipe_mnb_cvec = Pipeline([('cvec', CountVectorizer()),\n",
    "                         ('mnb', MultinomialNB())])\n",
    "\n",
    "# Define dictionary of hyperparameters.\n",
    "pipe_mnb_cvec_params = {\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'cvec__max_features': [40_000, 80_000] #reduce the amount of features as there are alot of features at 160,000\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV object.\n",
    "pipe_mnb_cvec_gs = GridSearchCV(pipe_mnb_cvec,\n",
    "                               pipe_mnb_cvec_params,\n",
    "                               cv=5,\n",
    "                               verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:06:28.173737Z",
     "start_time": "2021-01-27T07:06:20.351782Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV object to the data.\n",
    "pipe_mnb_cvec_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f'Finished and best params: {pipe_mnb_cvec_gs.best_params_} and best score: {pipe_mnb_cvec_gs.best_score_}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:13.424709Z",
     "start_time": "2021-01-27T08:01:12.258362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate default CountVectorizer\n",
    "mnb_cvec = CountVectorizer(max_features=80_000, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the CountVectorizer to train\n",
    "mnb_Xtrain_cvec = pd.DataFrame(mnb_cvec.fit_transform(\n",
    "    Xtrain).toarray(), columns=mnb_cvec.get_feature_names())\n",
    "\n",
    "# Transform the fitted CountVectorizer to val\n",
    "mnb_Xval_cvec = pd.DataFrame(mnb_cvec.transform(\n",
    "    Xval).toarray(), columns=mnb_cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:43:59.440658Z",
     "start_time": "2021-01-27T07:06:43.945384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate LogisticRegression model\n",
    "mnb_cvec_ = MultinomialNB()\n",
    "\n",
    "# fit train cvec data to LogisticRegression model\n",
    "mnb_cvec_model = mnb_cvec_.fit(mnb_Xtrain_cvec, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T07:44:43.124476Z",
     "start_time": "2021-01-27T07:44:43.103819Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "#pickle.dump(mnb_cvec_model, open('../models/mnb_cvec_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:15.249577Z",
     "start_time": "2021-01-27T08:01:15.228890Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the model from disk in case crash\n",
    "#mnb_cvec_model = pickle.load(open('../models/mnb_cvec_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model similar to their LogisticRegression counterpart, performed much better than the baseline but performed slightly worse than their counterparts at 82% accuracy and is also slightly overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:01:59.571035Z",
     "start_time": "2021-01-27T08:01:17.882218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8845603215681955\n",
      "0.8046745953088866\n"
     ]
    }
   ],
   "source": [
    "# check the scoring for the train set\n",
    "mnb_cvec_train_score = mnb_cvec_model.score(mnb_Xtrain_cvec, ytrain)\n",
    "print(mnb_cvec_train_score)\n",
    "\n",
    "# check the scoring for the validation set\n",
    "print(mnb_cvec_model.score(mnb_Xval_cvec, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:02:20.114190Z",
     "start_time": "2021-01-27T08:02:09.553703Z"
    }
   },
   "outputs": [],
   "source": [
    "# predictions of the validation data\n",
    "mnb_cvec_ypred = mnb_cvec_model.predict(mnb_Xval_cvec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 socre for the mnb_cvec_model, Multinomial Naive Bayes model fitted with CountVectorized features, is 0.72 which sligtly lower than the lr_cvec and lr_tfidf models before similar trend as the accuracy score. F1score will be the main metrics for measuring and comparing the models as this is an imbalance multiclass text classification. Another metric to compare and see the performance of the models is to look at the confusion matrix itself. We can see from the confusion matrix that it also follows a similar trend with the lr_cvec and lr_tfidf models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:02:49.360444Z",
     "start_time": "2021-01-27T08:02:49.307844Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.80\n",
      "Macro Recall: 0.66\n",
      "Macro F1-score: 0.70\n",
      "\n",
      "Accuracy: 0.80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.713     0.803     0.756      1002\n",
      "           2      0.789     0.109     0.192       137\n",
      "           3      0.893     0.594     0.713       379\n",
      "           4      0.901     0.520     0.660       421\n",
      "           5      0.952     0.673     0.788       263\n",
      "           6      0.752     0.309     0.438       285\n",
      "           7      0.831     0.954     0.888      1091\n",
      "           8      0.958     0.947     0.952       243\n",
      "           9      0.798     0.705     0.749       465\n",
      "          10      0.000     0.000     0.000         9\n",
      "          11      0.863     0.938     0.899      1641\n",
      "          12      0.845     0.551     0.667       227\n",
      "          13      0.705     0.844     0.768      1343\n",
      "          14      0.916     0.624     0.742       279\n",
      "          15      0.765     0.811     0.787       523\n",
      "          16      0.883     0.913     0.898       862\n",
      "          17      0.930     0.882     0.905       771\n",
      "          18      0.919     0.674     0.778       270\n",
      "          19      0.723     0.921     0.810      1566\n",
      "          20      0.801     0.390     0.524       331\n",
      "\n",
      "    accuracy                          0.805     12108\n",
      "   macro avg      0.797     0.658     0.696     12108\n",
      "weighted avg      0.814     0.805     0.794     12108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/becksphan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/becksphan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Call scoring function\n",
    "scoring(yval, mnb_cvec_ypred, 'MultinomialNB-CVEC model', mnb_cvec_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:02:23.482539Z",
     "start_time": "2021-01-27T08:02:23.337Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb_cvec_cm = pd.DataFrame(sklearn.metrics.confusion_matrix(yval, mnb_cvec_ypred))\n",
    "mnb_cvec_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNaiveBayes Model - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer although similar to CountVectorizer taking into account the number of times each word appears in a document, but uses the term frequency and puts weights on each word for a document. Lets see how well it performs in comparison to the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:04:04.498589Z",
     "start_time": "2021-01-27T08:04:04.491769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create pipe\n",
    "pipe_mnb_tfidf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('mnb', MultinomialNB())])\n",
    "\n",
    "# Define dictionary of hyperparameters.\n",
    "pipe_mnb_tfidf_params = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__max_features': [40_000, 80_000]}\n",
    "\n",
    "# Instantiate GridSearchCV object.\n",
    "pipe_mnb_tfidf_gs = GridSearchCV(pipe_mnb_tfidf,\n",
    "                                pipe_mnb_tfidf_params,\n",
    "                                cv=5,\n",
    "                                verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:04:13.787046Z",
     "start_time": "2021-01-27T08:04:06.059113Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 1) ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 1) ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 1) ............\n",
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 1) ............\n",
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 1) ............\n",
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 2), total=   0.5s\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 2), total=   0.5s\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 2), total=   0.5s\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 2), total=   0.5s\n",
      "[CV] tfidf__max_features=40000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=40000, tfidf__ngram_range=(1, 2), total=   0.5s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 1) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 1) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 1) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 1) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 1) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 2), total=   0.5s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 2), total=   0.5s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 2), total=   0.5s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 2), total=   0.5s\n",
      "[CV] tfidf__max_features=80000, tfidf__ngram_range=(1, 2) ............\n",
      "[CV]  tfidf__max_features=80000, tfidf__ngram_range=(1, 2), total=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    7.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and best params: {'tfidf__max_features': 40000, 'tfidf__ngram_range': (1, 1)} and best score: 0.7412588037510119.\n"
     ]
    }
   ],
   "source": [
    "# Fit the GridSearchCV object to the data.\n",
    "pipe_mnb_tfidf_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f'Finished and best params: {pipe_mnb_tfidf_gs.best_params_} and best score: {pipe_mnb_tfidf_gs.best_score_}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:04:32.483672Z",
     "start_time": "2021-01-27T08:04:31.822240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate default TfidfVectorizer\n",
    "mnb_tfidf = TfidfVectorizer(max_features=40_000, ngram_range=(1,1))\n",
    "\n",
    "# Fit and transform the CountVectorizer to train\n",
    "mnb_Xtrain_tfidf = pd.DataFrame(mnb_tfidf.fit_transform(\n",
    "    Xtrain).toarray(), columns=mnb_tfidf.get_feature_names())\n",
    "\n",
    "# Transform the fitted CountVectorizer to val\n",
    "mnb_Xval_tfidf = pd.DataFrame(mnb_tfidf.transform(\n",
    "    Xval).toarray(), columns=mnb_tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:04:35.408616Z",
     "start_time": "2021-01-27T08:04:34.056782Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate LogisticRegression model\n",
    "mnb_tfidf_ = MultinomialNB()\n",
    "\n",
    "# fit train cvec data to LogisticRegression model\n",
    "mnb_tfidf_model = mnb_tfidf_.fit(mnb_Xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:04:38.269433Z",
     "start_time": "2021-01-27T08:04:38.263636Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(mnb_tfidf_model, open('../models/mnb_tfidf_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk in case crash\n",
    "#mnb_tfidf_model = pickle.load(open('../models/mnb_tfidf_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model also performed much better than the baseline but performed slightly worse than their counterparts at 81% accuracy and is more overfitted than the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:04:45.516050Z",
     "start_time": "2021-01-27T08:04:44.580348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7902923847805737\n",
      "0.7558638916418896\n"
     ]
    }
   ],
   "source": [
    "# check the scoring for the train set\n",
    "mnb_tfidf_train_score = mnb_tfidf_model.score(mnb_Xtrain_tfidf, ytrain)\n",
    "print(mnb_tfidf_train_score)\n",
    "\n",
    "# check the scoring for the validation set\n",
    "print(mnb_tfidf_model.score(mnb_Xval_tfidf, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:04:47.199547Z",
     "start_time": "2021-01-27T08:04:46.990025Z"
    }
   },
   "outputs": [],
   "source": [
    "# predictions of the validation data\n",
    "mnb_tfidf_ypred = mnb_tfidf_model.predict(mnb_Xval_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 socre for the mnb_tfidf_model, Multinomial Naive Bayes model fitted with TfidfVectorized features, is 0.70 which lower than the models before except for the lr_d2v model. This is a similar trend with its accuracy score. F1score will be the main metrics for measuring and comparing the models as this is an imbalance multiclass text classification. Another metric to compare and see the performance of the models is to look at the confusion matrix itself. We can see from the confusion matrix that it also follows a similar trend with the lr_cvec and lr_tfidf models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T08:04:48.463341Z",
     "start_time": "2021-01-27T08:04:48.417909Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.78\n",
      "Macro Recall: 0.57\n",
      "Macro F1-score: 0.62\n",
      "\n",
      "Accuracy: 0.76\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.645     0.782     0.707      1002\n",
      "           2      0.750     0.022     0.043       137\n",
      "           3      0.901     0.507     0.649       379\n",
      "           4      0.894     0.442     0.591       421\n",
      "           5      0.968     0.574     0.721       263\n",
      "           6      0.667     0.140     0.232       285\n",
      "           7      0.797     0.942     0.864      1091\n",
      "           8      0.990     0.840     0.909       243\n",
      "           9      0.814     0.604     0.694       465\n",
      "          10      0.000     0.000     0.000         9\n",
      "          11      0.821     0.935     0.875      1641\n",
      "          12      0.849     0.348     0.494       227\n",
      "          13      0.617     0.840     0.712      1343\n",
      "          14      0.949     0.401     0.564       279\n",
      "          15      0.773     0.717     0.744       523\n",
      "          16      0.891     0.855     0.873       862\n",
      "          17      0.928     0.842     0.883       771\n",
      "          18      0.964     0.496     0.655       270\n",
      "          19      0.644     0.925     0.759      1566\n",
      "          20      0.835     0.260     0.396       331\n",
      "\n",
      "    accuracy                          0.756     12108\n",
      "   macro avg      0.785     0.574     0.618     12108\n",
      "weighted avg      0.782     0.756     0.737     12108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/becksphan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/becksphan/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Call scoring function\n",
    "scoring(yval, mnb_tfidf_ypred, 'MultinomialNB-TFIDF model', mnb_tfidf_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T09:08:19.397850Z",
     "start_time": "2021-01-25T09:08:19.366774Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb_tfidf_cm = pd.DataFrame(sklearn.metrics.confusion_matrix(yval, mnb_tfidf_ypred))\n",
    "mnb_tfidf_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNaiveBayes Model - Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec creates vectors in dimension space of negative values. Therefore unable to fit into a multinomial Naive Bayes model. In this case we shall use Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-27T08:05:22.052Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] doc2vec__vector_size=100 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... doc2vec__vector_size=100, total= 2.0min\n",
      "[CV] doc2vec__vector_size=100 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "gnb_d2v_params = {'doc2vec__vector_size': [100,200]\n",
    "}\n",
    "\n",
    "gnb_d2v_pipe = Pipeline([('doc2vec', Doc2VecModel(workers=cores, dm=0)), ('gnb', GaussianNB())])\n",
    "\n",
    "gnb_d2v_gs = GridSearchCV(gnb_d2v_pipe, \n",
    "                        param_grid=gnb_d2v_params,\n",
    "                        verbose=2,\n",
    "                        cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the data.\n",
    "gnb_d2v_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f'Finished and best params: {gnb_d2v_gs.best_params_} and best score: {gnb_d2v_gs.best_score_}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Doc2Vec model\n",
    "gnb_model_d2v = Doc2Vec(vector_size=200, dm=0, min_count=5, workers=cores)\n",
    "gnb_model_d2v.build_vocab([x for x in tqdm(train_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Doc2Vec model\n",
    "%time\n",
    "for epoch in range(30):\n",
    "    gnb_model_d2v.train(train_tagged, total_examples=gnb_model_d2v.corpus_count, epochs=gnb_model_d2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract training vectors\n",
    "gnb_Xtrain_d2v = np.array([gnb_model_d2v.docvecs[str(i)] for i in range(len(train_tagged))])\n",
    "gnb_Xtrain_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for test, but for val, we would want to generate vectors for prediction\n",
    "gnb_Xval_d2v = np.array([gnb_model_d2v.infer_vector(val_tagged[i][0]) for i in range(len(val_tagged))])\n",
    "gnb_Xval_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Model\n",
    "gnb_d2v = GaussianNB()\n",
    "gnb_d2v_model = gnb_d2v.fit(gnb_Xtrain_d2v, ytrain)\n",
    "gnb_d2v_train_score = gnb_d2v_model.score(gnb_Xtrain_d2v, ytrain)\n",
    "gnb_d2v_ypred = gnb_d2v_model.predict(gnb_Xval_d2v)\n",
    "print(gnb_d2v_train_score)\n",
    "print('Testing accuracy %s' % accuracy_score(yval, gnb_ypred_d2v))\n",
    "print('Testing F1 score: {}'.format(f1_score(yval, gnb_ypred_d2v, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(gnb_d2v_model, open('../models/gnb_d2v_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "#gnb_d2v_model = pickle.load(open('../models/gnb_d2v_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call scoring function\n",
    "scoring(yval, gnb_ypred_d2v, 'GaussianNB-D2V model', gnb_d2v_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_d2v_cm = pd.DataFrame(sklearn.metrics.confusion_matrix(yval, gnb_d2v_ypred))\n",
    "gnb_d2v_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model - CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees.\n",
    "[Wiki](https://en.wikipedia.org/wiki/Random_forest#:~:text=Random%20forests%20or%20random%20decision,average%20prediction%20(regression)%20of%20the)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T13:20:22.634891Z",
     "start_time": "2021-01-25T13:20:22.628885Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create pipe\n",
    "pipe_rf_cvec = Pipeline([('cvec', CountVectorizer()),\n",
    "                         ('rf', RandomForestClassifier(random_state=42, criterion='gini', n_estimators=100))]) \n",
    "# Gini Impurity is a measurement of the likelihood of an incorrect classification uses less computational needs than entropy\n",
    "# Entropy is the measures of impurity, disorder or uncertainty in a bunch of examples\n",
    "\n",
    "# Define dictionary of hyperparameters.\n",
    "pipe_rf_cvec_params = {\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'cvec__min_df': [3, 5],\n",
    "    'cvec__max_features': [20_000, 40_000] #reduce the amount of features as there are alot of features at 160,000\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV object.\n",
    "pipe_rf_cvec_gs = GridSearchCV(pipe_rf_cvec,\n",
    "                               pipe_rf_cvec_params,\n",
    "                               cv=5,\n",
    "                               verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T16:17:47.648734Z",
     "start_time": "2021-01-25T13:20:25.682204Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV object to the data.\n",
    "pipe_rf_cvec_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f'Finished and best params: {pipe_rf_cvec_gs.best_params_} and best score: {pipe_rf_cvec_gs.best_score_}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T16:18:24.939211Z",
     "start_time": "2021-01-25T16:18:23.963637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate default CountVectorizer\n",
    "rf_cvec = CountVectorizer(max_features=20_000, min_df=3, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the CountVectorizer to train\n",
    "rf_Xtrain_cvec = pd.DataFrame(rf_cvec.fit_transform(\n",
    "    Xtrain).toarray(), columns=rf_cvec.get_feature_names())\n",
    "\n",
    "# Transform the fitted CountVectorizer to val\n",
    "rf_Xval_cvec = pd.DataFrame(rf_cvec.transform(\n",
    "    Xval).toarray(), columns=rf_cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T08:56:02.132433Z",
     "start_time": "2021-01-25T08:49:07.020715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate LogisticRegression model\n",
    "rf_cvec_ = RandomForestClassifer(n_estimators=100, random_state=42, criterion='gini')\n",
    "\n",
    "# fit train cvec data to LogisticRegression model\n",
    "rf_cvec_model = rf_cvec_.fit(rf_Xtrain_cvec, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(rf_cvec_model, open('../models/rf_cvec_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "#rf_cvec_model = pickle.load(open('../models/rf_cvec_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model similar to their LogisticRegression counterpart, performed much better than the baseline but performed slightly worse than their counterparts at 82% accuracy and is also slightly overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T08:56:09.953819Z",
     "start_time": "2021-01-25T08:56:02.163668Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the scoring for the train set\n",
    "rf_cvec_train_score = rf_cvec_model.score(rf_Xtrain_cvec, ytrain)\n",
    "print(rf_cvec_train_score)\n",
    "\n",
    "# check the scoring for the validation set\n",
    "print(rf_cvec_model.score(rf_Xval_cvec, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T08:56:10.811268Z",
     "start_time": "2021-01-25T08:56:09.962603Z"
    }
   },
   "outputs": [],
   "source": [
    "# predictions of the validation data\n",
    "rf_cvec_ypred = rf_cvec_model.predict(rf_Xval_cvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 socre for the mnb_cvec_model, Multinomial Naive Bayes model fitted with CountVectorized features, is 0.72 which sligtly lower than the lr_cvec and lr_tfidf models before similar trend as the accuracy score. F1score will be the main metrics for measuring and comparing the models as this is an imbalance multiclass text classification. Another metric to compare and see the performance of the models is to look at the confusion matrix itself. We can see from the confusion matrix that it also follows a similar trend with the lr_cvec and lr_tfidf models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T08:56:10.869852Z",
     "start_time": "2021-01-25T08:56:10.814337Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Call scoring function\n",
    "scoring(yval, rf_cvec_ypred, 'RandomForest-CVEC model', rf_cvec_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T08:56:10.913051Z",
     "start_time": "2021-01-25T08:56:10.872180Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_cvec_cm = pd.DataFrame(sklearn.metrics.confusion_matrix(yval, rf_cvec_ypred))\n",
    "rf_cvec_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer although similar to CountVectorizer taking into account the number of times each word appears in a document, but uses the term frequency and puts weights on each word for a document. Lets see how well it performs in comparison to the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T08:57:04.193023Z",
     "start_time": "2021-01-25T08:57:04.130967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create pipe\n",
    "pipe_rf_tfidf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('rf', RandomForestClassifier(random_state=42, criterion='gini', n_estimators=100))]) \n",
    "\n",
    "# Define dictionary of hyperparameters.\n",
    "pipe_rf_tfidf_params = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__max_features': [20_000, 40_000] #reduce the amount of features as there are alot of features at 160,000\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV object.\n",
    "pipe_rf_tfidf_gs = GridSearchCV(pipe_rf_tfidf,\n",
    "                                pipe_rf_tfidf_params,\n",
    "                                cv=5,\n",
    "                                verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T08:57:37.445727Z",
     "start_time": "2021-01-25T08:57:11.332221Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV object to the data.\n",
    "pipe_rf_tfidf_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f'Finished and best params: {pipe_rf_tfidf_gs.best_params_} and best score: {pipe_rf_tfidf_gs.best_score_}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T09:07:56.651283Z",
     "start_time": "2021-01-25T09:07:55.627049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate default TfidfVectorizer\n",
    "rf_tfidf = TfidfVectorizer(max_features=40_000, min_df=3, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the CountVectorizer to train\n",
    "rf_Xtrain_tfidf = pd.DataFrame(rf_tfidf.fit_transform(\n",
    "    Xtrain).toarray(), columns=rf_tfidf.get_feature_names())\n",
    "\n",
    "# Transform the fitted CountVectorizer to val\n",
    "rf_Xval_tfidf = pd.DataFrame(rf_tfidf.transform(\n",
    "    Xval).toarray(), columns=rf_tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T09:08:03.194603Z",
     "start_time": "2021-01-25T09:07:57.936924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate LogisticRegression model\n",
    "rf_tfidf_ = MultinomialNB(n_estimators=, random_state=42, criterion='gini')\n",
    "\n",
    "# fit train cvec data to LogisticRegression model\n",
    "rf_tfidf_model = rf_tfidf_.fit(mnb_Xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(rf_tfidf_model, open('../models/rf_tfidf_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "#rf_tfidf_model = pickle.load(open('../models/rf_tfidf_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model also performed much better than the baseline but performed slightly worse than their counterparts at 81% accuracy and is more overfitted than the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T09:08:07.551837Z",
     "start_time": "2021-01-25T09:08:05.606288Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the scoring for the train set\n",
    "rf_tfidf_train_score = rf_tfidf_model.score(rf_Xtrain_tfidf, ytrain)\n",
    "print(rf_tfidf_train_score)\n",
    "\n",
    "# check the scoring for the validation set\n",
    "print(rf_tfidf_model.score(rf_Xval_tfidf, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T09:08:08.841693Z",
     "start_time": "2021-01-25T09:08:08.549753Z"
    }
   },
   "outputs": [],
   "source": [
    "# predictions of the validation data\n",
    "rf_tfidf_ypred = rf_tfidf_model.predict(rf_Xval_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 socre for the mnb_tfidf_model, Multinomial Naive Bayes model fitted with TfidfVectorized features, is 0.70 which lower than the models before except for the lr_d2v model. This is a similar trend with its accuracy score. F1score will be the main metrics for measuring and comparing the models as this is an imbalance multiclass text classification. Another metric to compare and see the performance of the models is to look at the confusion matrix itself. We can see from the confusion matrix that it also follows a similar trend with the lr_cvec and lr_tfidf models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T09:08:10.360165Z",
     "start_time": "2021-01-25T09:08:10.306342Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Call scoring function\n",
    "scoring(yval, rf_tfidf_ypred, 'RandomForest-TFIDF model', rf_tfidf_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T09:08:19.397850Z",
     "start_time": "2021-01-25T09:08:19.366774Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_tfidf_cm = pd.DataFrame(sklearn.metrics.confusion_matrix(yval, rf_tfidf_ypred))\n",
    "rf_tfidf_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model - Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:07:37.442037Z",
     "start_time": "2021-01-26T01:41:17.886677Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_d2v_params = {'doc2vec__vector_size': [100,200],\n",
    "                 'doc2vec__min_count': [2, 3]\n",
    "}\n",
    "\n",
    "rf_d2v_pipe = Pipeline([('doc2vec', Doc2VecModel(workers=cores, dm=0)), ('rf', RandomForestClassifier(random_state=42, criterion='gini', n_estimators=100))])\n",
    "\n",
    "rf_d2v_gs = GridSearchCV(rf_d2v_pipe, \n",
    "                        param_grid=rf_d2v_params,\n",
    "                        verbose=2,\n",
    "                        cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the data.\n",
    "rf_d2v_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f'Finished and best params: {rf_d2v_gs.best_params_} and best score: {rf_d2v_gs.best_score_}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Doc2Vec model\n",
    "rf_model_d2v = Doc2Vec(size=100, dm=1, min_count=2, workers=cores)\n",
    "rf_model_d2v.build_vocab([x for x in tqdm(train_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Doc2Vec model\n",
    "%time\n",
    "for epoch in range(30):\n",
    "    rf_model_d2v.train(train_tagged, total_examples=rf_model_d2v.corpus_count, epochs=rf_model_d2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract training vectors\n",
    "rf_Xtrain_d2v = np.array([rf_model_d2v.docvecs[str(i)] for i in range(len(train_tagged))])\n",
    "rf_Xtrain_d2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for test, but for val, we would want to generate vectors for prediction\n",
    "rf_Xval_d2v = np.array([rf_model_d2v.infer_vector(val_tagged[i][0]) for i in range(len(val_tagged))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Model\n",
    "rf_d2v = RandomForestClassifier(n_estimators=, random_state=42, criterion='gini')\n",
    "rf_d2v_model = rf_d2v.fit(rf_Xtrain_d2v, ytrain)\n",
    "rf_d2v_train_score = rf_d2v_model.score(rf_Xtrain_d2v, ytrain)\n",
    "rf_d2v_ypred = rf_d2v_model.predict(rf_Xval_d2v)\n",
    "print('Testing accuracy %s' % accuracy_score(yval, rf_ypred_d2v))\n",
    "print('Testing F1 score: {}'.format(f1_score(yval, rf_ypred_d2v, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(rf_d2v_model, open('../models/rf_d2v_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "#rf_d2v_model = pickle.load(open('../models/rf_d2v_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call scoring function\n",
    "scoring(yval, rf_ypred_d2v, 'RandomForest-D2V model', rf_d2v_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_d2v_cm = pd.DataFrame(sklearn.metrics.confusion_matrix(yval, rf_d2v_ypred))\n",
    "rf_d2v_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Long Short-Term Memory Model - CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the simpler models seem to do pretty well, I decided to try deep learning CNN. CNN and all deep learning neural networks are known more for sequential data, like data with time related factor or even images or text reviews that have some word meaning to it. The results are much worse performing than those simple models which is to be expected for a simple product name prediction, where words may not hold that much meaning with each other. Deep learning tends to require more computational power which means more resources. Therefore, since the results are not great, it is makes more economical sense to choose a simpler model.\n",
    "[Deep Learning](https://towardsdatascience.com/text-classification-rnns-or-cnn-s-98c86a0dd361)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T00:58:40.090714Z",
     "start_time": "2021-01-27T00:58:39.180119Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize \n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(Xtrain)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(Xtrain)\n",
    "X_test = tokenizer.texts_to_sequences(Xval)\n",
    "\n",
    "#build vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(Xtrain[:1])\n",
    "print(X_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T00:59:51.234399Z",
     "start_time": "2021-01-27T00:59:51.072226Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad to get same length for all documents/sentence\n",
    "maxlen = 10\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:03:31.125045Z",
     "start_time": "2021-01-27T01:03:31.092729Z"
    }
   },
   "outputs": [],
   "source": [
    "#build model\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:31:23.784775Z",
     "start_time": "2021-01-27T01:24:38.060842Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit model\n",
    "history = model.fit(X_train, ytrain,\n",
    "                    epochs=50,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test, yval),\n",
    "                    batch_size=10)\n",
    "loss_train, accuracy_train = model.evaluate(X_train, ytrain, verbose=2)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy_train))\n",
    "loss_val, accuracy_val = model.evaluate(X_test, yval, verbose=2)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:54:27.759630Z",
     "start_time": "2021-01-27T01:54:27.531245Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot loss during training\n",
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.tight_layout(h_pad=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:41:30.792027Z",
     "start_time": "2021-01-27T01:41:30.714359Z"
    }
   },
   "outputs": [],
   "source": [
    "#try with max pooling -> like undersampling\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:47:38.321724Z",
     "start_time": "2021-01-27T01:41:32.103240Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit model again\n",
    "history = model.fit(X_train, ytrain,\n",
    "                    epochs=50,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test, yval),\n",
    "                    batch_size=10)\n",
    "loss_train, accuracy_train = model.evaluate(X_train, ytrain, verbose=2)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy_train))\n",
    "loss_val, accuracy_val = model.evaluate(X_test, yval, verbose=2)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:54:12.709217Z",
     "start_time": "2021-01-27T01:54:12.465172Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot loss during training\n",
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.tight_layout(h_pad=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of production model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(model_name, model_precision, model_recall, model_F1scorse, model_train,accuracy, model_test_accuracy)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try plotting all the f1 macro score and vline benchmark and baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=model_df.Accuracy, y=model_df.index, orient='h',color='lightgrey')\n",
    "plt.vlines(x=0.13, ymin=0, ymax=2.5, colors='red', linestyles='dashed', label='baseline')\n",
    "plt.title('Accuracy Comparison for Selected Model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at why some of the products got misclassified. Looking at the confusion matrix, most of the products were predicted into the correct class/aisles. However, there are about 20% products misclassified. Lets deep dive into the products that were misclassified quite substantially. For example, products from juice nectars got misclassified to refrigerated and vice versa quite substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.746775Z",
     "start_time": "2021-01-23T04:28:34.710Z"
    }
   },
   "outputs": [],
   "source": [
    "# taking the index of validation set from main df and all the columns after it for aisles\n",
    "benchmark_val_df = Train.loc[yval.index,:]\n",
    "print(yval.shape)\n",
    "print(baseline_val_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.749137Z",
     "start_time": "2021-01-23T04:28:34.712Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_val_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.753856Z",
     "start_time": "2021-01-23T04:28:34.715Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping irrelevant columns\n",
    "baseline_val_df.drop(columns=['aisle_id','aisle_name'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.755484Z",
     "start_time": "2021-01-23T04:28:34.717Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_val_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.757479Z",
     "start_time": "2021-01-23T04:28:34.719Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting the aisle names of validation set to add into confusion matrix\n",
    "aisle_names = baseline_val_df.groupby(['aisle_id'])['aisle_name'].value_counts()\n",
    "aisle_names.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.761503Z",
     "start_time": "2021-01-23T04:28:34.724Z"
    }
   },
   "outputs": [],
   "source": [
    "aislelist = []\n",
    "for i, name in aisle_names.index:\n",
    "    if name not in aislelist:\n",
    "        aislelist.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.763889Z",
     "start_time": "2021-01-23T04:28:34.726Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of aisles in the validation set\n",
    "aislelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.766002Z",
     "start_time": "2021-01-23T04:28:34.728Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(28, 28))\n",
    "cm = plot_confusion_matrix(baseline_model,Xaisles_val_cvec,yaisles_val, ax=ax, cmap=plt.cm.Blues)\n",
    "ax.set_xticklabels(aislelist,rotation=90)\n",
    "ax.set_yticklabels(aislelist,rotation=0)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.767423Z",
     "start_time": "2021-01-23T04:28:34.731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the predicted aisles\n",
    "baseline_val_df['pred'] = ypred_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.769591Z",
     "start_time": "2021-01-23T04:28:34.733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get only products which got misclassified\n",
    "baseline_misclass_df = baseline_val_df[baseline_val_df['aisle_id']!=baseline_val_df['pred']]\n",
    "misclass = baseline_misclass_df.shape\n",
    "val = baseline_val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.774519Z",
     "start_time": "2021-01-23T04:28:34.735Z"
    }
   },
   "outputs": [],
   "source": [
    "#finding the percentage of misclassification\n",
    "print(misclass)\n",
    "print(val)\n",
    "misclass[0]/val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.776047Z",
     "start_time": "2021-01-23T04:28:34.737Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_misclass_df[baseline_misclass_df['aisle_name']=='refrigerated'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the aisles juice nectars and refrigerated is very similar, and some misclassifications still makes sense, for example, ginger kombucha could be classified as refrigerated as well even though the actual is juice nectars whereas organic amber coconut nectar could be juice nectars even though the actual is refrigerated. However, it does seem that the product name is being classified according to the fruit name in each product name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.777413Z",
     "start_time": "2021-01-23T04:28:34.740Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_misclass_df[(baseline_misclass_df['aisle_name']=='juice nectars') & (baseline_misclass_df['pred']==31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.779768Z",
     "start_time": "2021-01-23T04:28:34.743Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_misclass_df[(baseline_misclass_df['aisle_name']=='refrigerated') & (baseline_misclass_df['pred']==98)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.781306Z",
     "start_time": "2021-01-23T04:28:34.745Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_val_df[(baseline_val_df['aisle_name']=='refrigerated') & (baseline_val_df['pred']==31)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T04:56:09.782986Z",
     "start_time": "2021-01-23T04:28:34.748Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_val_df[(baseline_val_df['aisle_name']=='juice nectars') & (baseline_val_df['pred']==98)].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could try showing t-sne on how well it was clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
